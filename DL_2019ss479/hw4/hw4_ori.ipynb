{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 479: Deep Learning (Spring 2019)  \n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n",
    "Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat479-ss2019/  \n",
    "GitHub repository: https://github.com/rasbt/stat479-deep-learning-ss19\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rH4XmErYj5wm"
   },
   "source": [
    "# Homework 4: Implementing a Convolutional Neural Network (40 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this 4th homework, your task is to implement a convolutional neural network for classifying images in the CIFAR-10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html). \n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "- The CIFAR-10 dataset contains 60,000 color images with pixel dimensions 32x32. \n",
    "- There are 50,000 training images and 10,000 test images\n",
    "- Shown below is a snapshot showing a random selection for the 10 different object classes (from https://www.cs.toronto.edu/~kriz/cifar.html):\n",
    "\n",
    "![](cifar-snapshot.png)\n",
    "\n",
    "The CIFAR-10 dataset is already made accessible via the PyTorch API as it is a common dataset for benchmarking image classifiers. Hence, you do not have to download the dataset manually -- it will be downloaded automatically when you call\n",
    "\n",
    "```python\n",
    "train_dataset = datasets.CIFAR10(root='data', \n",
    "                                 train=True, \n",
    "                                 transform=transforms.ToTensor(),\n",
    "                                 download=True)\n",
    "```\n",
    "\n",
    "in the provided code cells below for the first time. Thus, keep in mind that calling this function for the first time may be a bit slow depending on your internet connection. On a conventional internet connection, it should be downloaded in a matter of seconds, though.\n",
    "\n",
    "Note that we are **not** using a separate validation dataset in this homework for tuning this network. This is intentional for the purposes of simplicity. However, in a real-world application, you are highly advised to use a validation dataset to tune the hyper parameters of the network as discussed in class.\n",
    "\n",
    "### Your Tasks\n",
    "\n",
    "Your main task is to implement a simple convolutional neural network that is loosely inspired by the AlexNet architecture that one the ImageNet competition in 2012: \n",
    "\n",
    "- Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). [Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks) (pp. 1097-1105).\n",
    "\n",
    "Then, you will make several simple modifications to this network architecture to improve its performance.\n",
    "\n",
    "Note that in this homework, as explained above, you will NOT be working with ImageNet but CIFAR-10, which is a much smaller dataset, in order for you to be able to train the network in a timely manner.\n",
    "\n",
    "In particular, you will be asked to first implement a basic convolutional neural network based on AlexNet and then make several improvements to optimize the performance and reduce overfitting. These \"improvements\" include Dropout, BatchNorm, and image augmentation, which will serve as a good exercise for familiarizing yourself with \"Deep Learning Tricks\" as well as convolutional neural networks.\n",
    "\n",
    "Note that the homework is relatively easy and straightforward, but the training of the network in each of the 5 sections will take ~5 min to train on a GPU. On a CPU, it will probably be much longer. Because training on the CPU will take much longer, and because you probably don't want your computer to overheat, I **highly recommend running this homework on a cloud server**, for example, Google Colab (which allows you to use a GPU for free). Since you don't have to download the dataset manually, it should be relatively straightforward to do this homework on Google Colab and then download the solution for submission via Canvas. Please let me know if you have any questions about that. For a Cloud computing refresher, please see: https://github.com/rasbt/stat479-deep-learning-ss19/tree/master/L07_cloud-computing.\n",
    "\n",
    "**The due date for this homework is Friday, 11 April 11:59 pm.** Please start as soon as possible, because while this homework is conceptually straightforward, you have to factor in the computation time (~30 min runtime for the complete notebook if you run this notebook with correct solutions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MkoGLH_Tj5wn"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ORj09gnrj5wp"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### NO NEED TO CHANGE THIS CELL\n",
    "###############################\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-9ce5eb0f967e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reload_ext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'watermark'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'watermark'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-a 'Sebastian_Raschka' -ud -iv \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2305\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2306\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2307\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2308\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<C:\\Users\\Oliver\\Anaconda3\\lib\\site-packages\\decorator.py:decorator-gen-127>\u001b[0m in \u001b[0;36mwatermark\u001b[1;34m(self, line)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\watermark\\watermark.py\u001b[0m in \u001b[0;36mwatermark\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_git_branch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miversions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print_all_import_versions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatermark\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\watermark\\watermark.py\u001b[0m in \u001b[0;36m_print_all_import_versions\u001b[1;34m(vars)\u001b[0m\n\u001b[0;32m    245\u001b[0m                             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[0mlongest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mto_print\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mto_print\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlongest\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian_Raschka' -ud -iv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6hghKPxj5w0"
   },
   "source": [
    "## Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23936,
     "status": "ok",
     "timestamp": 1524974497505,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "NnT0sZIwj5wu",
    "outputId": "55aed925-d17e-4c6a-8c71-0d9b3bde5637"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### NO NEED TO CHANGE THIS CELL\n",
    "###############################\n",
    "\n",
    "\n",
    "#-------------------------\n",
    "### SETTINGS\n",
    "#-------------------------\n",
    "\n",
    "# Hyperparameters\n",
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Architecture\n",
    "NUM_FEATURES = 32*32\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Other\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell that implements the ResNet-34 architecture is a derivative of the code provided at https://pytorch.org/docs/0.4.0/_modules/torchvision/models/resnet.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RAodboScj5w6"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170500096it [00:51, 7680327.53it/s]                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([256, 3, 32, 32])\n",
      "Image label dimensions: torch.Size([256])\n",
      "Image batch dimensions: torch.Size([256, 3, 32, 32])\n",
      "Image label dimensions: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### NO NEED TO CHANGE THIS CELL\n",
    "###############################\n",
    "\n",
    "#-------------------------\n",
    "### CIFAR-10 Dataset\n",
    "#-------------------------\n",
    "\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.CIFAR10(root='data', \n",
    "                                 train=True, \n",
    "                                 transform=transforms.ToTensor(),\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='data', \n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          num_workers=8,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=8,\n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cfc4861860>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tran(test_dataset[10][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cfc48b4e10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tran(train_dataset[10][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### NO NEED TO CHANGE THIS CELL\n",
    "###############################\n",
    "\n",
    "def compute_epoch_loss(model, data_loader):\n",
    "    model.eval()\n",
    "    curr_loss, num_examples = 0., 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            logits, probas = model(features)\n",
    "            loss = F.cross_entropy(logits, targets, reduction='sum')\n",
    "            num_examples += targets.size(0)\n",
    "            curr_loss += loss\n",
    "\n",
    "        curr_loss = curr_loss / num_examples\n",
    "        return curr_loss\n",
    "\n",
    "\n",
    "def compute_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_lza9t_uj5w1"
   },
   "source": [
    "## 1) Implement a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will be implementing the AlexNet-variant that you will be using and modifying throughout this homework. On purpose, this will be a bit more \"hands-off\" than usual, so that you get a chance to practice implementing neural networks from scratch based on sketches and short descriptions (which is a useful real-world skill as it is quite common to reimplement architectures from literature in order to verify results and compare those architectures to your own methods).\n",
    "\n",
    "The architecture is as follows:\n",
    "\n",
    "![](architecture-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I made this network based on AlexNet, as mentioned in the introduction, but there are some differences. Overall though, there are 7 hidden layers in total: 5 convolutional layers and 2 fully-connected layers. There is one output layers mapping the last layer's activations to the classes. For this network, \n",
    "\n",
    "- all hidden layers are connected via ReLU activation functions\n",
    "- the output layer uses a softmax activation function\n",
    "- make sure you return the logits and the softmax output; the logits are used for computing the cross-entropy loss (instead of passing the softmax outputs) for numerical stability reasons as discussed in earlier lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "class ConvNet1(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet1, self).__init__()\n",
    "\n",
    "        self.conv1 =  nn.Conv2d(3, 64,    kernel_size=5, stride=1, padding=2)\n",
    "        self.\n",
    "        \n",
    "        # ... <add the remaining convolutional layers\n",
    "        # and fully connected layers ...\n",
    "        \n",
    "        self.linear3 = nn.Linear(4096, num_classes)  \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # ... IMPLEMENT FORWARD PASS ...\n",
    "        logits = # ...\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "    \n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model1 = ConvNet1(NUM_CLASSES)\n",
    "model1.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=LEARNING_RATE)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1547
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2384585,
     "status": "ok",
     "timestamp": 1524976888520,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "Dzh3ROmRj5w7",
    "outputId": "5f8fd8c9-b076-403a-b0b7-fd2d498b48d7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### NO NEED TO CHANGE THIS CELL\n",
    "###############################\n",
    "\n",
    "def train(model, train_loader, test_loader):\n",
    "\n",
    "    minibatch_cost, epoch_cost = [], []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "\n",
    "            features = features.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "\n",
    "            ### FORWARD AND BACK PROP\n",
    "            logits, probas = model(features)\n",
    "            cost = F.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            cost.backward()\n",
    "            minibatch_cost.append(cost)\n",
    "\n",
    "            ### UPDATE MODEL PARAMETERS\n",
    "            optimizer.step()\n",
    "\n",
    "            ### LOGGING\n",
    "            if not batch_idx % 150:\n",
    "                print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                       %(epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                         len(train_loader), cost))\n",
    "\n",
    "    \n",
    "        with torch.set_grad_enabled(False): # save memory during inference\n",
    "            print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
    "                  epoch+1, NUM_EPOCHS, \n",
    "                  compute_accuracy(model, train_loader, device=DEVICE)))\n",
    "            \n",
    "            cost = compute_epoch_loss(model, train_loader)\n",
    "            epoch_cost.append(cost)\n",
    "\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))\n",
    "\n",
    "    print('Total Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "    return minibatch_cost, epoch_cost\n",
    "    \n",
    "\n",
    "minibatch_cost, epoch_cost = train(model1, train_loader, test_loader)\n",
    "\n",
    "\n",
    "plt.plot(range(len(minibatch_cost)), minibatch_cost)\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Minibatch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model1  # to save memory if you don't use it anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Adding Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part, your task is now to add dropout layers to reduce overfitting. You can copy&paste your architecture from above and make the appropriate modifications. In particular,\n",
    "\n",
    "- place a Dropout2d (this is also referred to as \"spatial dropout\"; will be explained in the lecture) before each maxpooling layer with dropout probability p=0.2,\n",
    "- place a regular dropout after each fully connected layer with probability p=0.5, except for the last (output) layer.\n",
    "\n",
    "The architecture is as follows (changes, compared to the previous section, are highlighted in red):\n",
    "\n",
    "![](architecture-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "class ConvNet2(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64,    kernel_size=5, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, stride = 1, padding = 2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.dropout2 = nn.Dropout2d(0.2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.dropout3 = nn.Dropout2d(0.2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size = 5, stride = 1, padding = 2)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        self.dropout4 = nn.Dropout2d(0.2)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size = 5, stride = 1, padding = 2)\n",
    "        self.pool5 = nn.MaxPool2d(2)\n",
    "        self.dropout5 = nn.Dropout2d(0.2)\n",
    "        \n",
    "        \n",
    "        self.linear1 = nn.Linear(256, 4096)\n",
    "        self.lin_drop_1 = nn.Dropout(0.5)\n",
    "        self.linear2 = nn.Linear(4096, 4096)\n",
    "        self.lin_drop_2 = nn.Dropout(0.5)\n",
    "        self.linear3 = nn.Linear(4096, num_classes) \n",
    "        #### YOUR CODE\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "         # ... IMPLEMENT FORWARD PASS ...\n",
    "        out_1 = self.conv1(x)\n",
    "        out_1 = torch.relu(out_1)\n",
    "        out_1 = self.pool1(out_1)\n",
    "        \n",
    "        out_2 = self.conv2(out_1)\n",
    "        out_2 = self.dropout2(out_2)\n",
    "        out_2 = torch.relu(out_2)\n",
    "        out_2 = self.pool2(out_2)\n",
    "        \n",
    "        out_3 = self.conv3(out_2)\n",
    "        out_3 = self.dropout3(out_3)\n",
    "        out_3 = torch.relu(out_3)\n",
    "        out_3 = self.pool3(out_3)\n",
    "        \n",
    "        out_4 = self.conv4(out_3)\n",
    "        out_4 = self.dropout4(out_4)\n",
    "        out_4 = torch.relu(out_4)\n",
    "        out_4 = self.pool4(out_4)\n",
    "        \n",
    "        out_5 = self.conv5(out_4)\n",
    "        out_5 = self.dropout5(out_5)\n",
    "        out_5 = torch.relu(out_5)\n",
    "        out_5 = self.pool5(out_5)\n",
    "        #print(out_5.size())\n",
    "        out_5 = out_5.view(-1,256)\n",
    "        #print(out_5.size())\n",
    "        \n",
    "        out_6 = self.linear1(out_5)\n",
    "        out_6 = torch.relu(out_6)\n",
    "        out_6 = self.lin_drop_1(out_6)\n",
    "        \n",
    "        out_7 = self.linear2(out_6)\n",
    "        out_7 = torch.relu(out_7)\n",
    "        out_7 = self.lin_drop_2(out_7)\n",
    "        \n",
    "        out_8 = self.linear3(out_7)\n",
    "        \n",
    "        \n",
    "        logits = out_8\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "\n",
    "    \n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model2 = ConvNet2(NUM_CLASSES)\n",
    "model2.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "minibatch_cost, epoch_cost = train(model2, train_loader, test_loader)\n",
    "\n",
    "\n",
    "plt.plot(range(len(minibatch_cost)), minibatch_cost)\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Minibatch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model2  # to save memory if you don't use it anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Add BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this 3rd part, you are now going to add BatchNorm layers to further improve the performance of the network. This use BatchNorm2D for the convolutional layers and BatchNorm1D for the fully connected layers.\n",
    "\n",
    "\n",
    "The architecture is as follows (changes, compared to the previous section, are highlighted in red):\n",
    "\n",
    "![](architecture-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "class ConvNet3(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet3, self).__init__()\n",
    "        \n",
    "        #### YOUR CODE\n",
    "        self.conv1 = nn.Conv2d(3, 64,    kernel_size=5, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, stride = 1, padding = 2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.dropout2 = nn.Dropout2d(0.2)\n",
    "        self.batch2 = nn.BatchNorm2d(192)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.dropout3 = nn.Dropout2d(0.2)\n",
    "        self.batch3 = nn.BatchNorm2d(384)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size = 5, stride = 1, padding = 2)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        self.dropout4 = nn.Dropout2d(0.2)\n",
    "        self.batch4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size = 5, stride = 1, padding = 2)\n",
    "        self.pool5 = nn.MaxPool2d(2)\n",
    "        self.dropout5 = nn.Dropout2d(0.2)\n",
    "        self.batch5 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        \n",
    "        self.linear1 = nn.Linear(256, 4096)\n",
    "        self.lin_drop_1 = nn.Dropout(0.5)\n",
    "        self.lin_batch_1 = nn.BatchNorm1d(4096)\n",
    "        \n",
    "        self.linear2 = nn.Linear(4096, 4096)\n",
    "        self.lin_drop_2 = nn.Dropout(0.5)\n",
    "        self.lin_batch_2 = nn.BatchNorm1d(4096)\n",
    "        \n",
    "        self.linear3 = nn.Linear(4096, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #### YOUR CODE\n",
    "        out_1 = self.conv1(x)\n",
    "        out_1 = torch.relu(out_1)\n",
    "        out_1 = self.pool1(out_1)\n",
    "        \n",
    "        out_2 = self.conv2(out_1)\n",
    "        out_2 = self.batch2(out_2)\n",
    "        out_2 = self.dropout2(out_2)\n",
    "        out_2 = torch.relu(out_2)\n",
    "        out_2 = self.pool2(out_2)\n",
    "        \n",
    "        out_3 = self.conv3(out_2)\n",
    "        out_3 = self.batch3(out_3)\n",
    "        out_3 = self.dropout3(out_3)\n",
    "        out_3 = torch.relu(out_3)\n",
    "        out_3 = self.pool3(out_3)\n",
    "        \n",
    "        out_4 = self.conv4(out_3)\n",
    "        out_4 = self.batch4(out_4)\n",
    "        out_4 = self.dropout4(out_4)\n",
    "        out_4 = torch.relu(out_4)\n",
    "        out_4 = self.pool4(out_4)\n",
    "        \n",
    "        out_5 = self.conv5(out_4)\n",
    "        out_5 = self.batch5(out_5)\n",
    "        out_5 = self.dropout5(out_5)\n",
    "        out_5 = self.pool5(out_5)\n",
    "        out_5 = torch.relu(out_5)\n",
    "        #print(out_5.size())\n",
    "        out_5 = out_5.view(-1,256)\n",
    "        #print(out_5.size())\n",
    "        \n",
    "        out_6 = self.linear1(out_5)\n",
    "        out_6 = self.lin_batch_1(out_6)\n",
    "        out_6 = torch.relu(out_6)\n",
    "        out_6 = self.lin_drop_1(out_6)\n",
    "        \n",
    "        out_7 = self.linear2(out_6)\n",
    "        out_7 = self.lin_batch_2(out_7)\n",
    "        out_7 = torch.relu(out_7)\n",
    "        out_7 = self.lin_drop_2(out_7)\n",
    "        \n",
    "        out_8 = self.linear3(out_7)\n",
    "        \n",
    "        \n",
    "        logits = out_8\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "\n",
    "    \n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model3 = ConvNet3(NUM_CLASSES)\n",
    "model3.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "minibatch_cost, epoch_cost = train(model3, train_loader, test_loader)\n",
    "\n",
    "\n",
    "plt.plot(range(len(minibatch_cost)), minibatch_cost)\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Minibatch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model3  # to save memory if you don't use it anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Going All-Convolutional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this 4th part, your task is to remove all maxpooling layers and replace the fully-connected layers by convolutional layers. Note that the number of elements of the activation tensors in the hidden layers should not change. I.e., when you remove the max-pooling layers, you need to increase the stride of the convolutional layers from 1 to 2 to achieve the same scaling. Furthermore, you can replace a fully-connected conmvolutional layer by a convolutional layer using stride=1 and a kernel with height and width equal to 1.\n",
    "\n",
    "The new architecture is as follows (changes, compared to the previous section, are highlighted in red):\n",
    "\n",
    "![](architecture-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "class ConvNet4(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet4, self).__init__()\n",
    "        \n",
    "        #### YOUR CODE\n",
    "        self.conv1 = nn.Conv2d(3, 64,  kernel_size=5, stride=2, padding=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, stride = 2, padding = 2)\n",
    "        self.dropout2 = nn.Dropout2d(0.2)\n",
    "        self.batch2 = nn.BatchNorm2d(192)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.dropout3 = nn.Dropout2d(0.2)\n",
    "        self.batch3 = nn.BatchNorm2d(384)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size = 5, stride = 2, padding = 2)\n",
    "        self.dropout4 = nn.Dropout2d(0.2)\n",
    "        self.batch4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size = 5, stride = 2, padding = 2)\n",
    "        self.dropout5 = nn.Dropout2d(0.2)\n",
    "        self.batch5 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(256,4096,kernal_size = 1, stride = 1)\n",
    "        self.lin_drop_1 = nn.Dropout2d(0.5)\n",
    "        self.lin_batch_1 = nn.BatchNorm2d(4096)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(4096, 4096, kernel_size = 1, stride = 1)\n",
    "        self.lin_drop_2 = nn.Dropout2d(0.5)\n",
    "        self.lin_batch_2 = nn.BatchNorm2d(4096)\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(4096, num_classes, kernal_size = 1, stride = 1) \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #### YOUR CODE\n",
    "        out_1 = self.conv1(x)\n",
    "        out_1 = torch.relu(out_1)\n",
    "        \n",
    "        out_2 = self.conv2(out_1)\n",
    "        out_2 = self.batch2(out_2)\n",
    "        out_2 = self.dropout2(out_2)\n",
    "        out_2 = torch.relu(out_2)\n",
    "        \n",
    "        out_3 = self.conv3(out_2)\n",
    "        out_3 = self.batch3(out_3)\n",
    "        out_3 = self.dropout3(out_3)\n",
    "        out_3 = torch.relu(out_3)\n",
    "        \n",
    "        out_4 = self.conv4(out_3)\n",
    "        out_4 = self.batch4(out_4)\n",
    "        out_4 = self.dropout4(out_4)\n",
    "        out_4 = torch.relu(out_4)\n",
    "        \n",
    "        out_5 = self.conv5(out_4)\n",
    "        out_5 = self.batch5(out_5)\n",
    "        out_5 = self.dropout5(out_5)\n",
    "        out_5 = torch.relu(out_5)\n",
    "        #print(out_5.size())\n",
    "        #out_5 = out_5.view(-1,256)\n",
    "        #print(out_5.size())\n",
    "        \n",
    "        out_6 = self.conv6(out_5)\n",
    "        out_6 = self.lin_batch_1(out_6)\n",
    "        out_6 = torch.relu(out_6)\n",
    "        out_6 = self.lin_drop_1(out_6)\n",
    "        \n",
    "        out_7 = self.conv7(out_6)\n",
    "        out_7 = self.lin_batch_2(out_7)\n",
    "        out_7 = torch.relu(out_7)\n",
    "        out_7 = self.lin_drop_2(out_7)\n",
    "        \n",
    "        out_8 = self.conv8(out_7)\n",
    "        \n",
    "        \n",
    "        x = out_8\n",
    "\n",
    "        #x = self.conv8(x)\n",
    "        \n",
    "        logits = x.view(x.size(0), NUM_CLASSES)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "    \n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model4 = ConvNet4(NUM_CLASSES)\n",
    "model4.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model4.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "minibatch_cost, epoch_cost = train(model4, train_loader, test_loader)\n",
    "\n",
    "\n",
    "plt.plot(range(len(minibatch_cost)), minibatch_cost)\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Minibatch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Add Image Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last section, you should use the architecture from the previous section (section 4) but use additional image augmentation during training to improve the generalization performance. \n",
    "\n",
    "\n",
    "In particular, you should modify the `train_transform = transforms.Compose([...`) function so that it\n",
    "\n",
    "- performs a random horizontal flip with propbability 50%\n",
    "- resizes the image from 32x32 to 40x40\n",
    "- performs a 32x32 random crop from the 40x40 images\n",
    "- normalizes the pixel intensities such that they are within the range [-1, 1]\n",
    "\n",
    "The `test_transform = transforms.Compose([...` function should be modified accordingly, such that it \n",
    "\n",
    "- resizes the image from 32x32 to 40x40\n",
    "- performs a 32x32 **center** crop from the 40x40 images\n",
    "- normalizes the pixel intensities such that they are within the range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize(40),\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x:(x - min(x))/(x.max() - x.min()))\n",
    "    #### YOUR CODE\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(40),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.Lambda(lambda x:(x - min(x))/(x.max() - x.min()))\n",
    "        #### YOUR CODE\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='data', \n",
    "                                 train=True, \n",
    "                                 transform=train_transform,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='data', \n",
    "                                train=False, \n",
    "                                transform=test_transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          num_workers=8,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=8,\n",
    "                         shuffle=False)\n",
    "\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model4 = ConvNet4(NUM_CLASSES)\n",
    "model4.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model4.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "minibatch_cost, epoch_cost = train(model4, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model1, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Optional: Training the network for 200 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this optional section, train the network from the previous part for 200 epochs to see how it performs as the training loss converges. This will take about 50 minutes and is optional (you will not receive a penalty if you don't run this section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 200\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model4 = ConvNet4(NUM_CLASSES)\n",
    "model4.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model4.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "minibatch_cost, epoch_cost = train(model4, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(minibatch_cost)), minibatch_cost)\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Minibatch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(epoch_cost)), epoch_cost)\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions (Your Answers Required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you implemented the AlexNet-like architecture and made several modifications to it, please report the number of learnable parameters for each model, i.e., the weights and biases and batchnorm parameters, etc. (excluding the parameters of the ADAM optimizer). Also, please paste the training and test set accuracies below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- **Model from section 1)**\n",
    "    - Train accuracy: 98.440%\n",
    "    - Test accuracy: 71.98%\n",
    "\n",
    "Number of learnable parameters: (include your computation to receive partial points if the final answer is wrong)\n",
    "\n",
    "[insert computation and answer]\n",
    "\n",
    "\n",
    "- Conv2d (1) 5 * 5 * 3 * 64 + 64 = 4864\n",
    "- Conv2d (2) 5 * 5 * 64 * 192 + 192 = 307392\n",
    "- Conv2d (3) 3 * 3 * 192 * 384 + 384 = 663936\n",
    "- Conv2d (4) 5 * 5 * 384 * 256 + 256 = 2457856\n",
    "- Conv2d (5) 5 * 5 * 256 * 256 + 256 = 1638656\n",
    "- FC (1)     4096 * 256 + 4096 = 1052672\n",
    "- FC (2)     4096 * 4096 + 4096 = 16781312\n",
    "- FC (3)     4096 * 10 + 10 = 40970\n",
    "- Total number of parameters: 4864 + 307392 + 663936 + 2457856 + 1638656 + 1052672 + 16781312 + 40970 = 22947658\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22947658"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4864 + 307392 + 663936 + 2457856 + 1638656 + 1052672 + 16781312 + 40970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model from section 2)**\n",
    "    - Train accuracy: 94.112%\n",
    "    - Test accuracy: 75.81%\n",
    "\n",
    "Number of learnable parameters: (include your computation to receive partial points if the final answer is wrong)\n",
    "\n",
    "[insert computation and answer]\n",
    "\n",
    "\n",
    "- Conv2d (1) 5 * 5 * 3 * 64 + 64 = 4864\n",
    "- Conv2d (2) 5 * 5 * 64 * 192 + 192 = 307392\n",
    "- Conv2d (3) 3 * 3 * 192 * 384 + 384 = 663936\n",
    "- Conv2d (4) 5 * 5 * 384 * 256 + 256 = 2457856\n",
    "- Conv2d (5) 5 * 5 * 256 * 256 + 256 = 1638656\n",
    "- FC (1)     4096 * 256 + 4096 = 1052672\n",
    "- FC (2)     4096 * 4096 + 4096 = 16781312\n",
    "- FC (3)     4096 * 10 + 10 = 40970\n",
    "- Total number of parameters: 4864 + 307392 + 663936 + 2457856 + 1638656 + 1052672 + 16781312 + 40970 = 22947658\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model from section 3)**\n",
    "    - Train accuracy: 98.468%\n",
    "    - Test accuracy: 81.24%\n",
    "\n",
    "Number of learnable parameters: (include your computation to receive partial points if the final answer is wrong)\n",
    "\n",
    "[insert computation and answer]\n",
    "\n",
    "\n",
    "- Conv2d (1) 5 * 5 * 3 * 64 + 64 = 4864\n",
    "- Conv2d (2) 5 * 5 * 64 * 192 + 192 + 192*2 = 307776\n",
    "- Conv2d (3) 3 * 3 * 192 * 384 + 384 + 384*2 = 664704\n",
    "- Conv2d (4) 5 * 5 * 384 * 256 + 256 + 256*2 = 2458368\n",
    "- Conv2d (5) 5 * 5 * 256 * 256 + 256 + 256*2 = 1639168\n",
    "- FC (1)     4096 * 256 + 4096 + 4096*2 = 1060864\n",
    "- FC (2)     4096 * 4096 + 4096 + 4096*2 = 116789504\n",
    "- FC (3)     4096 * 10 + 10 = 40970\n",
    "- Total number of parameters: 4864 + 307776 + 664704 + 2458368 + 1639168 + 1060864 + 16789504 + 40970 = 22966218\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22966218"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4864 + 307776 + 664704 + 2458368 + 1639168 + 1060864 + 16789504 + 40970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model from section 4)**\n",
    "    - Train accuracy: 98.876%\n",
    "    - Test accuracy: 75.00%\n",
    "\n",
    "Number of learnable parameters: (include your computation to receive partial points if the final answer is wrong)\n",
    "\n",
    "[insert computation and answer]\n",
    "\n",
    "\n",
    "- Conv2d (1) 5 * 5 * 3 * 64 + 64 = 4864\n",
    "- Conv2d (2) 5 * 5 * 64 * 192 + 192 + 192*2 = 307776\n",
    "- Conv2d (3) 3 * 3 * 192 * 384 + 384 + 384*2 = 664704\n",
    "- Conv2d (4) 5 * 5 * 384 * 256 + 256 + 256*2 = 2458368\n",
    "- Conv2d (5) 5 * 5 * 256 * 256 + 256 + 256*2 = 1639168\n",
    "- FC (1)     4096 * 256 + 4096 + 4096*2 = 1060864\n",
    "- FC (2)     4096 * 4096 + 4096 + 4096*2 = 116789504\n",
    "- FC (3)     4096 * 10 + 10 = 40970\n",
    "- Total number of parameters: 4864 + 307776 + 664704 + 2458368 + 1639168 + 1060864 + 16789504 + 40970 = 22966218\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model from section 5)**\n",
    "    - Train accuracy: 67.470%\n",
    "    - Test accuracy: 77.18%\n",
    "\n",
    "Number of learnable parameters: (include your computation to receive partial points if the final answer is wrong)\n",
    "\n",
    "[insert computation and answer]\n",
    "\n",
    "\n",
    "- Conv2d (1) 5 * 5 * 3 * 64 + 64 = 4864\n",
    "- Conv2d (2) 5 * 5 * 64 * 192 + 192 + 192*2 = 307776\n",
    "- Conv2d (3) 3 * 3 * 192 * 384 + 384 + 384*2 = 664704\n",
    "- Conv2d (4) 5 * 5 * 384 * 256 + 256 + 256*2 = 2458368\n",
    "- Conv2d (5) 5 * 5 * 256 * 256 + 256 + 256*2 = 1639168\n",
    "- FC (1)     4096 * 256 + 4096 + 4096*2 = 1060864\n",
    "- FC (2)     4096 * 4096 + 4096 + 4096*2 = 116789504\n",
    "- FC (3)     4096 * 10 + 10 = 40970\n",
    "- Total number of parameters: 4864 + 307776 + 664704 + 2458368 + 1639168 + 1060864 + 16789504 + 40970 = 22966218\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model from section 6) [optional]**\n",
    "    - Train accuracy: ???%\n",
    "    - Test accuracy: ???%\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "convnet-vgg16.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
